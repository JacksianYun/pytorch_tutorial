{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_CNN_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "U6yBMOwflMsR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**original Source : https://github.com/pytorch/examples/mnist/main.py**"
      ]
    },
    {
      "metadata": {
        "id": "U26CpjsFv2NC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "380c0891-e62d-4e2f-866e-ad2b4c2a0b6d"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x57910000 @  0x7f43eb3532a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jMRK4jp0v4-p",
        "colab_type": "code",
        "outputId": "d5139e49-2086-49e5-ed6a-9d600095d81d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5854, 0.4931, 0.5230],\n",
            "        [0.6491, 0.4863, 0.0886],\n",
            "        [0.3866, 0.3015, 0.6196],\n",
            "        [0.0560, 0.6150, 0.9908],\n",
            "        [0.0174, 0.3519, 0.5809]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aY48twNkv5XA",
        "colab_type": "code",
        "outputId": "1d03d1c1-e326-46b4-b488-21594bed68e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "EKZUMaCqsJtX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRctBhQOvxl2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7H97yhfSsYMu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Net class setting**"
      ]
    },
    {
      "metadata": {
        "id": "9VDLfTtZskrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gqtTqX8Qso1F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **train and test set**"
      ]
    },
    {
      "metadata": {
        "id": "6Updu3eLstwl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9EW6Xa1lvxl5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## def main"
      ]
    },
    {
      "metadata": {
        "id": "RxcNx6nwvxl5",
        "colab_type": "code",
        "outputId": "b97dc37b-6feb-470a-b97c-fb46101a7ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2465
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "use_cuda = True\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "device = torch.device(\"cuda\") \n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=10000, shuffle=True)\n",
        "\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "if (False):\n",
        "    torch.save(model.state_dict(),\"mnist_cnn.pt\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300039\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.438883\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.366815\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.327680\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.224536\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.337847\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.063525\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.118217\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.145159\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.118076\n",
            "\n",
            "Test set: Average loss: 0.1022, Accuracy: 9659/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.147755\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.099380\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.058941\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.065016\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.120885\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.034197\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.034373\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.038155\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.029101\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.037407\n",
            "\n",
            "Test set: Average loss: 0.0611, Accuracy: 9828/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.052600\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.098214\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.033623\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.067634\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.033637\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.029376\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.023331\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.063187\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.046670\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.016094\n",
            "\n",
            "Test set: Average loss: 0.0558, Accuracy: 9809/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.019452\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.023196\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.074958\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.026647\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.017207\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.015332\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.010571\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.066269\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.014510\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.060267\n",
            "\n",
            "Test set: Average loss: 0.0407, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010524\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.060829\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.064305\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.054307\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.027748\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.041386\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.054603\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.176213\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.072508\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.030525\n",
            "\n",
            "Test set: Average loss: 0.0378, Accuracy: 9872/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.126666\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.021731\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.038169\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.018460\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.064721\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.013431\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.042692\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.007986\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.033659\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.005587\n",
            "\n",
            "Test set: Average loss: 0.0338, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.030634\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.006778\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.074202\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.025016\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.005354\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.012877\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.037536\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.004560\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.073729\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.010191\n",
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 9878/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.004841\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.037337\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.055664\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.016112\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.020907\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.013251\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.088519\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.006997\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.080983\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.010034\n",
            "\n",
            "Test set: Average loss: 0.0392, Accuracy: 9881/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.085481\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.007797\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.011942\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.005353\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.006982\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.007443\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.005765\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.038956\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004996\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.080284\n",
            "\n",
            "Test set: Average loss: 0.0292, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.121599\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.027256\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001321\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.030311\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002376\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.011440\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.016085\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.013245\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.021286\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.004254\n",
            "\n",
            "Test set: Average loss: 0.0318, Accuracy: 9896/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1LnX3FbF93EZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gz-dM1Vh6V78",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "aa6c451e-e64c-468b-a3ed-5773b09198e4"
      },
      "cell_type": "code",
      "source": [
        "idx = random.randrange(0,len(data))\n",
        "image = Variable(data[idx].view(-1,28*28).float())\n",
        "print('image idx : ', idx)\n",
        "print('target value :' ,target[idx].item())\n",
        "print('predict value :',pred[idx].item())\n",
        "plt.imshow(image.data.view(28,28).cpu().numpy(), cmap='gray')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image idx :  3140\n",
            "target value : 4\n",
            "predict value : 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3dddebecc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSAAqukgAKrpIACq6SAD/D5qWK8Pt3aU5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3dddf3fb70>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "R-7v3BZi1hmt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}